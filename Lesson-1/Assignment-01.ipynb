{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "1）文本分类\n",
    "2）机器翻译\n",
    "3）文本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "1）Jupyter对交互性较好，可以一条一条代码运行出结果。\n",
    "2）Pycharm兼容性较好，适用于项目开发。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "概率模型是描述不同随机变量之间关系的数学模型，通常情况下刻画了一个或多个随机变量之间的相互非确定性的概率关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "1）朴素贝叶斯\n",
    "2）语言模型\n",
    "3）高斯分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "概率能帮助我们弄清不确定时间发生的几率是多少。\n",
    "基于解析和模式匹配的编程的难点在于难以构建一个通用泛化的模型函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "语言模型是指一句话中的每个字或词组成一个句子的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "1）语音识别\n",
    "2）词性标注\n",
    "3）命名实体识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "每个字或词出现的概率，不与上下文有关，只与本身相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "劣势：\n",
    "1）无法识别语序颠倒的情况，即无语法结构。\n",
    "2）字或词之间没有关联，也没有约束。\n",
    "\n",
    "优势：\n",
    "1）概率独立分布，计算复杂度较低。\n",
    "2）词典的维度较小。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "每个字或词与它最近的一个字或词相关联，受到最近一个字或词的约束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_1 = '''\n",
    "语法 => 主语 谓语 宾语\n",
    "主语 => 你 | 我 | 他 | 她 | 它\n",
    "谓语 => 爱 | 喜欢 | 想 | 怀念\n",
    "宾语 => 你 | 我 | 他 | 她 | 它\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_2 = '''\n",
    "语法 => 时间 人物 助动词 地点 动作\n",
    "时间 => 昨天 时辰 | 今天 时辰 | 明天 时辰\n",
    "时辰 => null | 早上 | 中午 | 下午 | 晚上\n",
    "人物 => 我 | 你 | 他\n",
    "助动词 => 坐船去 | 坐火箭飞去 | 开车到 | 走路去\n",
    "地点 => 游乐场 | 学校 | 停车场 | 实验室 | 小岛 | 外太空\n",
    "动作 => 玩耍 | 学习 | 停车 | 做实验\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar(grammar_str, split='=>', line_split='\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):\n",
    "        if not line.strip():  # 去掉line前后空格后，line为空时，continue。\n",
    "            continue\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'语法': [['主语', '谓语', '宾语']], '主语': [['你'], ['我'], ['他'], ['她'], ['它']], '谓语': [['爱'], ['喜欢'], ['想'], ['怀念']], '宾语': [['你'], ['我'], ['他'], ['她'], ['它']]}\n"
     ]
    }
   ],
   "source": [
    "grammar_1 = create_grammar(grammar_1)\n",
    "print(grammar_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'语法': [['时间', '人物', '助动词', '地点', '动作']], '时间': [['昨天', '时辰'], ['今天', '时辰'], ['明天', '时辰']], '时辰': [['null'], ['早上'], ['中午'], ['下午'], ['晚上']], '人物': [['我'], ['你'], ['他']], '助动词': [['坐船去'], ['坐火箭飞去'], ['开车到'], ['走路去']], '地点': [['游乐场'], ['学校'], ['停车场'], ['实验室'], ['小岛'], ['外太空']], '动作': [['玩耍'], ['学习'], ['停车'], ['做实验']]}\n"
     ]
    }
   ],
   "source": [
    "grammar_2 = create_grammar(grammar_2)\n",
    "print(grammar_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(gram, target='语法'):\n",
    "    if target not in gram:\n",
    "        return target\n",
    "    import random\n",
    "    expaned = [generate(gram, t) for t in random.choice(gram[target])]\n",
    "    \n",
    "    return ''.join([e if e != '/n' else '\\n' for e in expaned if e != 'null'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天早上我开车到实验室学习\n"
     ]
    }
   ],
   "source": [
    "g = generate(gram=grammar, target='语法')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天早上我开车到外太空做实验\n",
      "今天中午他坐船去学校玩耍\n",
      "昨天他走路去外太空学习\n",
      "明天下午我走路去实验室做实验\n",
      "今天下午他开车到游乐场停车\n"
     ]
    }
   ],
   "source": [
    "def generate_n(n, gram=grammar, target='语法'):\n",
    "    for _ in range(n):\n",
    "        g = generate(gram, target)\n",
    "        print(g)\n",
    "\n",
    "generate_n(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    # we will learn the regular expression next course.\n",
    "    return re.findall('\\w+', string)\n",
    "\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                   吴京意淫到了脑残的地步，看了恶心想吐\n",
       "1    首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...\n",
       "2    吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...\n",
       "3                        凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。\n",
       "4                                                 中二得很\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'movie_comments.csv'\n",
    "\n",
    "content = pd.read_csv(filename, encoding='utf-8')\n",
    "content['comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = content['comment'].tolist()\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n"
     ]
    }
   ],
   "source": [
    "articles_clean = [''.join(token(str(a)))for a in articles]\n",
    "\n",
    "with open('articles_clean.txt', 'w') as f:\n",
    "    for i, a in enumerate(articles_clean):\n",
    "        if i % 50000 == 0:\n",
    "            print(i)\n",
    "        f.write(a + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /var/folders/55/dcp8q8wx08ld4bfxx_9726d40000gn/T/jieba.cache\n",
      "Loading model cost 0.799 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n"
     ]
    }
   ],
   "source": [
    "TOKEN = []\n",
    "for i, line in enumerate(open('articles_clean.txt')):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    TOKEN += cut(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164393"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count = Counter(TOKEN)\n",
    "len(words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 328262),\n",
       " ('\\n', 261497),\n",
       " ('了', 102420),\n",
       " ('是', 73106),\n",
       " ('我', 50338),\n",
       " ('都', 36255),\n",
       " ('很', 34712),\n",
       " ('看', 34022),\n",
       " ('电影', 33675),\n",
       " ('也', 32065),\n",
       " ('和', 31290),\n",
       " ('在', 31245),\n",
       " ('不', 28435),\n",
       " ('有', 27939),\n",
       " ('就', 25685),\n",
       " ('人', 23909),\n",
       " ('好', 22858),\n",
       " ('啊', 20803),\n",
       " ('这', 17484),\n",
       " ('还', 17449),\n",
       " ('一个', 17343),\n",
       " ('你', 17282),\n",
       " ('还是', 16425),\n",
       " ('但', 15578),\n",
       " ('故事', 15010),\n",
       " ('没有', 14343),\n",
       " ('就是', 14007),\n",
       " ('喜欢', 13566),\n",
       " ('让', 13304),\n",
       " ('太', 12676),\n",
       " ('又', 11566),\n",
       " ('剧情', 11359),\n",
       " ('没', 10858),\n",
       " ('说', 10764),\n",
       " ('吧', 10747),\n",
       " ('他', 10675),\n",
       " ('不错', 10416),\n",
       " ('得', 10349),\n",
       " ('到', 10341),\n",
       " ('给', 10300),\n",
       " ('这个', 10058),\n",
       " ('上', 10054),\n",
       " ('被', 9939),\n",
       " ('对', 9824),\n",
       " ('最后', 9694),\n",
       " ('一部', 9693),\n",
       " ('片子', 9590),\n",
       " ('什么', 9571),\n",
       " ('能', 9532),\n",
       " ('与', 9168),\n",
       " ('多', 8977),\n",
       " ('可以', 8972),\n",
       " ('不是', 8811),\n",
       " ('最', 8669),\n",
       " ('觉得', 8626),\n",
       " ('中', 8446),\n",
       " ('导演', 8390),\n",
       " ('自己', 8354),\n",
       " ('拍', 8172),\n",
       " ('好看', 8085),\n",
       " ('要', 8081),\n",
       " ('真的', 7908),\n",
       " ('感觉', 7828),\n",
       " ('但是', 7723),\n",
       " ('里', 7655),\n",
       " ('那', 7503),\n",
       " ('有点', 7479),\n",
       " ('想', 7442),\n",
       " ('这部', 7433),\n",
       " ('会', 7429),\n",
       " ('去', 7295),\n",
       " ('把', 7151),\n",
       " ('着', 7058),\n",
       " ('这么', 6784),\n",
       " ('小', 6626),\n",
       " ('个', 6546),\n",
       " ('而', 6507),\n",
       " ('这样', 6471),\n",
       " ('真是', 6449),\n",
       " ('那么', 6431),\n",
       " ('这种', 6377),\n",
       " ('片', 6333),\n",
       " ('不过', 6292),\n",
       " ('挺', 6244),\n",
       " ('时候', 6216),\n",
       " ('更', 6143),\n",
       " ('比', 6094),\n",
       " ('却', 5990),\n",
       " ('爱', 5909),\n",
       " ('我们', 5875),\n",
       " ('大', 5773),\n",
       " ('像', 5704),\n",
       " ('虽然', 5633),\n",
       " ('演技', 5631),\n",
       " ('其实', 5573),\n",
       " ('看到', 5450),\n",
       " ('知道', 5384),\n",
       " ('再', 5352),\n",
       " ('演员', 5328),\n",
       " ('那个', 5123)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_1(word):\n",
    "    return words_count[word] / len(TOKEN)\n",
    "\n",
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2:\n",
    "        return words_count_2[word1+word2] / words_count[word1]\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i, _ in enumerate(TOKEN[:-2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count_2 = Counter(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probablity(sentence):\n",
    "    words = cut(sentence)\n",
    "    sentence_pro = 1\n",
    "    \n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        probability = prob_2(word, next_)\n",
    "        sentence_pro *= probability\n",
    "    \n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8600395709893428e-54"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_from_txt(txt):\n",
    "    TOKEN = []\n",
    "    for i, line in enumerate(open(txt)):\n",
    "        TOKEN += cut(line)\n",
    "    return TOKEN\n",
    "\n",
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2:\n",
    "        return words_count_2[word1+word2] / len(TOKEN_2_GRAM)\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)\n",
    "\n",
    "def get_probablity(sentence):\n",
    "    words = cut(sentence)\n",
    "    sentence_pro = 1\n",
    "    \n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        probability = prob_2(word, next_)\n",
    "        sentence_pro *= probability\n",
    "    return sentence_pro\n",
    "\n",
    "def create_grammar(grammar_str, split='=>', line_split='\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):\n",
    "        if not line.strip():  # 去掉line前后空格后，line为空时，continue。\n",
    "            continue\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammar\n",
    "\n",
    "def generate(gram, target='语法'):\n",
    "    if target not in gram:\n",
    "        return target\n",
    "    import random\n",
    "    expaned = [generate(gram, t) for t in random.choice(gram[target])]\n",
    "    \n",
    "    return ''.join([e if e != '/n' else '\\n' for e in expaned if e != 'null'])\n",
    "\n",
    "def generate_best(gram, n):\n",
    "    ls = []\n",
    "    for _ in range(n):\n",
    "        temp_s = generate(gram)\n",
    "        temp_p = get_probablity(temp_s)\n",
    "        ls.append((temp_s, temp_p))\n",
    "    for i, j in ls:\n",
    "        print(i, '此句子合理的概率为：', j)\n",
    "    print('\\n其中最合理的句子为：', sorted(ls, key=lambda x: x[1], reverse=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "昨天早上我走路去外太空玩耍 此句子合理的概率为： 3.588677504933481e-10\n",
      "今天你坐火箭飞去停车场做实验 此句子合理的概率为： 2.436856612675519e-14\n",
      "昨天中午你开车到小岛学习 此句子合理的概率为： 4.8447146316602e-10\n",
      "昨天下午他走路去学校玩耍 此句子合理的概率为： 5.383016257400222e-10\n",
      "今天晚上他坐火箭飞去学校玩耍 此句子合理的概率为： 8.122855375585064e-15\n",
      "明天晚上他开车到游乐场做实验 此句子合理的概率为： 1.4621139676053112e-13\n",
      "今天早上他开车到小岛停车 此句子合理的概率为： 2.290645215914988e-12\n",
      "昨天中午他开车到小岛停车 此句子合理的概率为： 3.229809754440133e-10\n",
      "今天下午我开车到游乐场玩耍 此句子合理的概率为： 5.81365755799224e-09\n",
      "明天下午你坐火箭飞去学校停车 此句子合理的概率为： 8.122855375585064e-15\n",
      "\n",
      "其中最合理的句子为： ('今天下午我开车到游乐场玩耍', 5.81365755799224e-09)\n"
     ]
    }
   ],
   "source": [
    "TOKEN = token_from_txt('grammar_2.txt')\n",
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i, _ in enumerate(TOKEN[:-2])]\n",
    "words_count_2 = Counter(TOKEN_2_GRAM)\n",
    "\n",
    "grammar_2 = '''\n",
    "语法 => 时间 人物 助动词 地点 动作\n",
    "时间 => 昨天 时辰 | 今天 时辰 | 明天 时辰\n",
    "时辰 => null | 早上 | 中午 | 下午 | 晚上\n",
    "人物 => 我 | 你 | 他\n",
    "助动词 => 坐船去 | 坐火箭飞去 | 开车到 | 走路去\n",
    "地点 => 游乐场 | 学校 | 停车场 | 实验室 | 小岛 | 外太空\n",
    "动作 => 玩耍 | 学习 | 停车 | 做实验\n",
    "'''\n",
    "grammar = create_grammar(grammar_2)\n",
    "\n",
    "generate_best(gram=grammar, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "1）需要大规模的语料库才能确保准确率的提升，在grammar_2.txt中多加些正确的例子。\n",
    "2）句子长度越长，句子合理的概率越小？\n",
    "3）可采用3-Gram进行尝试。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
